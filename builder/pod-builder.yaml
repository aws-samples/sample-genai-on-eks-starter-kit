apiVersion: v1
kind: Pod
metadata:
  name: builder
  namespace: vllm
  # namespace: sglang
  # namespace: tgi
spec:
  nodeSelector:
    karpenter.sh/nodepool: custom
    kubernetes.io/arch: amd64
    eks.amazonaws.com/instance-category: m
    eks.amazonaws.com/instance-generation: "7"
    karpenter.sh/capacity-type: on-demand
  containers:
    - name: ubuntu
      image: ubuntu
      command: ["sleep", "604800"]
      tty: true
      stdin: true
      env:
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-token
              key: token
        - name: HF_HOME
          value: /root/.cache/huggingface
      resources:
        requests:
          cpu: 3 #75%
          memory: 12Gi #75%
      volumeMounts:
        - name: huggingface-cache
          mountPath: /root/.cache/huggingface
        # - name: neuron-cache
        #   mountPath: /root/.cache/neuron
  volumes:
    - name: huggingface-cache
      persistentVolumeClaim:
        claimName: huggingface-cache
    # - name: neuron-cache
    #   persistentVolumeClaim:
    #     claimName: neuron-cache
  tolerations:
    - key: karpenter.sh/nodepool
      operator: Equal
      value: custom
      effect: NoSchedule
# Terminal 1
# apt update && DEBIAN_FRONTEND=noninteractive apt install -y python3-venv python3-full htop
# python3 -m venv ~/myenv
# source ~/myenv/bin/activate
# pip install -U "huggingface_hub[cli]"
# huggingface-cli download Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8
# huggingface-cli download openai/gpt-oss-120b
# huggingface-cli download moonshotai/Kimi-K2-Instruct
# Terminal 2
# cd /root/.cache/huggingface/hub
# htop
