apiVersion: v1
kind: Pod
metadata:
  name: builder-neuron
  namespace: vllm
  # namespace: sglang
  # namespace: tgi
spec:
  hostUsers: false
  nodeSelector:
    karpenter.sh/nodepool: custom
    karpenter.sh/capacity-type: on-demand
    node.kubernetes.io/instance-type: inf2.xlarge
  containers:
    - name: ubuntu
      image: public.ecr.aws/neuron/pytorch-inference-neuronx:2.8.0-neuronx-py311-sdk2.26.0-ubuntu22.04
      tty: true
      stdin: true
      env:
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-token
              key: token
        - name: HF_HOME
          value: /root/.cache/huggingface
      resources:
        requests:
          cpu: 3.6
          memory: 14Gi
          aws.amazon.com/neuroncore: 2
        limits:
          aws.amazon.com/neuroncore: 2
      volumeMounts:
        - name: huggingface-cache
          mountPath: /root/.cache/huggingface
        - name: shm
          mountPath: /dev/shm
        - name: neuron-cache
          mountPath: /root/.cache/neuron
  volumes:
    - name: huggingface-cache
      persistentVolumeClaim:
        claimName: huggingface-cache
    - name: shm
      emptyDir:
        medium: Memory
    - name: neuron-cache
      persistentVolumeClaim:
        claimName: neuron-cache
  tolerations:
    - key: karpenter.sh/nodepool
      operator: Equal
      value: custom
      effect: NoSchedule
    - key: aws.amazon.com/neuron
      operator: Exists
      effect: NoSchedule
# Terminal 1
# pip install optimum-neuron[neuronx]

# pip show neuronx-cc
# pip show optimum-neuron
# optimum-cli neuron cache lookup Qwen/Qwen3-8B

# curl -fsSL https://get.docker.com -o get-docker.sh
# sh get-docker.sh
