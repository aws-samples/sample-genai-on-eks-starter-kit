global:
  security:
    # -- Allow insecure images to use our ECR images.
    allowInsecureImages: true

{{#if @root.integration.o11y.mlflow}}
image:
  repository: public.ecr.aws/agentic-ai-platforms-on-k8s/litellm
  tag: v1.79.1-mlflow
{{/if}}

resources:
  requests:
    cpu: 1 
    memory: 2Gi
  limits:
    memory: 2Gi

redis:
  enabled: true
  image:
    registry: public.ecr.aws
    repository: agentic-ai-platforms-on-k8s/redis
    tag: 8.2.1-debian-12-r0
    pullPolicy: IfNotPresent
  master:
    resources: 
      requests:
        cpu: 125m
        memory: 256Mi
      limits:
        memory: 256Mi

postgresql:
  image:
    registry: public.ecr.aws
    repository: agentic-ai-platforms-on-k8s/postgresql
    tag: 17.5.0-debian-12-r8
    pullPolicy: IfNotPresent
  primary:
    resources: 
      requests:
        cpu: 125m
        memory: 256Mi
      limits:
        memory: 256Mi

serviceAccount:
  create: true

masterkey: {{{LITELLM_API_KEY}}}
envVars:
  # SERVER_ROOT_PATH: /litellm # Same as Ingress path
  # UI_BASE_PATH: /litellm/ui # Move UI path but still need to build Docker image https://docs.litellm.ai/docs/proxy/deploy#1-custom-server-root-path-proxy-base-url
  # DOCS_URL: /docs/ # Move docs path but not working yet since LiteLLM hardcodes Swagger resource paths https://github.com/BerriAI/litellm/blob/main/litellm/proxy/proxy_server.py#L794-L796
  # LITELLM_LOG: DEBUG
  UI_USERNAME: {{{LITELLM_UI_USERNAME}}}
  UI_PASSWORD: {{{LITELLM_UI_PASSWORD}}}
  {{#if @root.integration.o11y.langfuse}}
  LANGFUSE_HOST: http://langfuse-web.langfuse:3000
  LANGFUSE_PUBLIC_KEY: {{{LANGFUSE_PUBLIC_KEY}}}
  LANGFUSE_SECRET_KEY: {{{LANGFUSE_SECRET_KEY}}}
  {{/if}}
  {{#if @root.integration.o11y.mlflow}}
  MLFLOW_TRACKING_URI: http://mlflow.mlflow
  MLFLOW_TRACKING_USERNAME: {{{MLFLOW_USERNAME}}}
  MLFLOW_TRACKING_PASSWORD: {{{MLFLOW_PASSWORD}}}
  MLFLOW_EXPERIMENT_NAME: Default
  MLFLOW_EXPERIMENT_ID: 0
  {{/if}}
  {{#if @root.integration.o11y.phoenix}}
  PHOENIX_API_KEY: {{{PHOENIX_API_KEY}}}
  PHOENIX_COLLECTOR_ENDPOINT: http://phoenix-svc.phoenix:4317/v1/traces
  PHOENIX_COLLECTOR_HTTP_ENDPOINT: http://phoenix-svc.phoenix:6006/v1/traces
  {{/if}}

ingress:
  enabled: true
  className: {{#if DOMAIN}}shared-{{/if}}internet-facing-alb
  annotations:
    alb.ingress.kubernetes.io/target-type: ip
    {{#if DOMAIN}}
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}]'
    {{/if}}
  hosts:
    - paths:
        - path: /
          pathType: Prefix
      {{#if DOMAIN}}
      host: litellm.{{{DOMAIN}}}
      {{/if}}

proxy_config:
  general_settings:
    master_key: os.environ/PROXY_MASTER_KEY
    store_model_in_db: true
    store_prompts_in_spend_logs: true
  litellm_settings:
    {{#with integration.o11y.config}}
    {{#if callbacks}}    
    callbacks: {{{callbacks}}}
    {{/if}}
    {{#if success_callback}}    
    success_callback: {{{success_callback}}}
    {{/if}}
    {{#if failure_callback}}
    failure_callback: {{{failure_callback}}}
    {{/if}}
    {{/with}}
    redact_user_api_key_info: true
    turn_off_message_logging: false
  model_list:
    # Bedrock LLM Models
    {{#each integration.bedrock.llm}}
    - model_name: bedrock/{{{name}}}
      litellm_params:
        model: bedrock/{{{model}}}
        {{#if @root.integration.bedrock.region}}
        aws_region_name: {{{@root.integration.bedrock.region}}}
        {{/if}}
    {{/each}}
    # Bedrock Embedding Models
    {{#each integration.bedrock.embedding}}
    - model_name: bedrock/{{{name}}}
      litellm_params:
        model: bedrock/{{{model}}}
        {{#if @root.integration.bedrock.region}}
        aws_region_name: {{{@root.integration.bedrock.region}}}
        {{/if}}
    {{/each}}
    {{#each integration.llm-model.vllm}}
    - model_name: vllm/{{@key}}
      litellm_params:
        model: openai/{{@key}}
        api_key: fake-key
        api_base: http://{{@key}}.vllm:8000/v1
    {{/each}}
    # SGlang Models
    {{#each integration.llm-model.sglang}}
    - model_name: sglang/{{@key}}
      litellm_params:
        model: openai/{{@key}}
        api_key: fake-key
        api_base: http://{{@key}}.sglang:30000/v1
    {{/each}}
    # TGI Models
    {{#each integration.llm-model.tgi}}
    - model_name: tgi/{{@key}}
      litellm_params:
        model: openai/{{@key}}
        api_key: fake-key
        api_base: http://{{@key}}.tgi:80/v1
    {{/each}}
    # Ollama Models
    {{#each integration.llm-model.ollama}}
    - model_name: ollama/{{@key}}
      litellm_params:
        model: ollama/{{@key}}
        api_key: fake-key
        api_base: http://ollama.ollama:11434
        drop_params: true
    {{/each}}
    # TEI Models
    {{#each integration.embedding-model.tei}}
    - model_name: tei/{{@key}}
      litellm_params:
        model: openai/{{@key}}
        api_key: fake-key
        api_base: http://{{@key}}.tei:80/v1
    {{/each}}
  # MCP Servers
  {{#if integration.mcp-servers.length}}
  mcp_servers:
    {{#each integration.mcp-servers}}
    {{this}}:
      url: "http://{{this}}.mcp-server:8000/mcp"
      transport: "http"
      auth_type: "api_key"
      spec_version: "2025-03-26"
    {{/each}}
  {{/if}}
  # Guardrails
  guardrails:
    {{#with integration.guardrail}}
    {{#if bedrock}}
    - guardrail_name: "bedrock-guardrail"
      litellm_params:
        guardrail: bedrock
        mode: "post_call"
        default_on: true
        guardrailIdentifier: "{{{bedrock.id}}}"
        guardrailVersion: "{{{bedrock.version}}}"
        {{#if @root.integration.bedrock.region}}
        aws_region_name: {{{@root.integration.bedrock.region}}}
        {{/if}}
    {{/if}}    
    {{/with}}
