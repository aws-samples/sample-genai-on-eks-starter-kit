apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: qwen3-omni-30b-thinking
  namespace: vllm
spec:
  replicas: 1
  leaderWorkerTemplate:
    size: 2
    restartPolicy: RecreateGroupOnPodRestart
    leaderTemplate:
      metadata:
        labels:
          app: qwen3-omni-30b-thinking
          role: leader
      spec:
        securityContext:
          seccompProfile:
            type: RuntimeDefault
        automountServiceAccountToken: false
        nodeSelector:
          {{{KARPENTER_PREFIX}}}/instance-family: g6e
        containers:
          - name: vllm
            image: vllm/vllm-openai:nightly
            imagePullPolicy: IfNotPresent
            securityContext:
              allowPrivilegeEscalation: false
              capabilities:
                drop:
                  - NET_RAW
              seccompProfile:
                type: RuntimeDefault
            command:
              - sh
              - -c
              - |
                bash /vllm-workspace/examples/online_serving/multi-node-serving.sh leader --ray_cluster_size=$(LWS_GROUP_SIZE)
                python3 -m vllm.entrypoints.openai.api_server \
                --port 8000 \
                --model Qwen/Qwen3-Next-80B-A3B-Thinking-FP8 \
                --served-model-name=qwen3-omni-30b-thinking \
                --trust-remote-code \
                --gpu-memory-utilization=0.95 \
                --max-model-len=65536 \
                --tensor-parallel-size=1 \
                --pipeline-parallel-size=2 \
                --allowed-local-media-path=/
            env:
              - name: HUGGING_FACE_HUB_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: hf-token
                    key: token
              - name: PYTORCH_CUDA_ALLOC_CONF
                value: expandable_segments:True
            ports:
              - containerPort: 8000
            resources:
              requests:
                cpu: 3 #75%
                memory: 24Gi #75%
                nvidia.com/gpu: 1
              limits:
                nvidia.com/gpu: 1
            volumeMounts:
              - name: huggingface-cache
                mountPath: /root/.cache/huggingface
              - name: shm
                mountPath: /dev/shm
        volumes:
          - name: huggingface-cache
            persistentVolumeClaim:
              claimName: huggingface-cache
          - name: shm
            emptyDir:
              medium: Memory
        tolerations:
          - key: nvidia.com/gpu
            operator: Exists
            effect: NoSchedule
    workerTemplate:
      metadata:
        labels:
          app: qwen3-omni-30b-thinking
          role: worker
      spec:
        securityContext:
          seccompProfile:
            type: RuntimeDefault
        automountServiceAccountToken: false
        nodeSelector:
          {{{KARPENTER_PREFIX}}}/instance-family: g6e
        containers:
          - name: vllm
            image: vllm/vllm-openai:nightly
            imagePullPolicy: IfNotPresent
            securityContext:
              allowPrivilegeEscalation: false
              capabilities:
                drop:
                  - NET_RAW
              seccompProfile:
                type: RuntimeDefault
            command:
              - sh
              - -c
              - "bash /vllm-workspace/examples/online_serving/multi-node-serving.sh worker --ray_address=$(LWS_LEADER_ADDRESS)"
            env:
              - name: HUGGING_FACE_HUB_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: hf-token
                    key: token
              - name: PYTORCH_CUDA_ALLOC_CONF
                value: expandable_segments:True
            resources:
              requests:
                cpu: 3 #75%
                memory: 24Gi #75%
                nvidia.com/gpu: 1
              limits:
                nvidia.com/gpu: 1
            volumeMounts:
              - name: huggingface-cache
                mountPath: /root/.cache/huggingface
              - name: shm
                mountPath: /dev/shm
        volumes:
          - name: huggingface-cache
            persistentVolumeClaim:
              claimName: huggingface-cache
          - name: shm
            emptyDir:
              medium: Memory
        tolerations:
          - key: nvidia.com/gpu
            operator: Exists
            effect: NoSchedule
---
apiVersion: v1
kind: Service
metadata:
  name: qwen3-omni-30b-thinking
  namespace: vllm
spec:
  selector:
    app: qwen3-omni-30b-thinking
    role: leader
  ports:
    - name: http
      port: 8000
